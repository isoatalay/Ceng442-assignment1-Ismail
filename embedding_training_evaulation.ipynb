{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec3d2c3-fa02-45aa-b856-aa7b4479d8e9",
   "metadata": {},
   "source": [
    "After cleaning, normalizing and labeling data, they are ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883e5bc9-b05d-4b5d-8f8e-870ee511af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec & FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# The cleaned, normalized, labeled data files that are processed in \"data_prcocessing.ipynb\" \n",
    "files = [\n",
    "    \"data/clean/labeled-sentiment_2col.xlsx\",\n",
    "    \"data/clean/test__1__2col.xlsx\",\n",
    "    \"data/clean/train__3__2col.xlsx\",\n",
    "    \"data/clean/train-00000-of-00001_2col.xlsx\",\n",
    "    \"data/clean/merged_dataset_CSV__1__2col.xlsx\",\n",
    "]\n",
    "\n",
    "sentences = []\n",
    "for f in files:\n",
    "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
    "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
    "\n",
    "Path(\"embeddings\").mkdir(exist_ok=True)  # Create a folder named embeddings\n",
    "\n",
    "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5,\n",
    "               min_count=3, sg=1, negative=10, epochs=10, seed=42)  # Train Word2Vec\n",
    "w2v.save(\"embeddings/word2vec.model\")\n",
    "\n",
    "print(\"Saved Word2Vec model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c658ac0-10de-4ff9-83df-aeae066b386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FaxtText model\n"
     ]
    }
   ],
   "source": [
    "ft = FastText(sentences=sentences, vector_size=300, window=5,\n",
    "              min_count=3, sg=1, min_n=3, max_n=6, epochs=10, seed=42)  # Train FastText\n",
    "ft.save(\"embeddings/fasttext.model\")\n",
    "\n",
    "print(\"Saved FaxtText model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1403c6f-3259-4699-bfb7-7c08780322a3",
   "metadata": {},
   "source": [
    "Now, we will compare **Word2Vec** vs **FastText** using metrics like:  \n",
    "- Coverage  \n",
    "- Synonym/Antonym similarity\n",
    "- Nearest-neighbor quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b45cc96-4a44-4717-a61b-863e057f5ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Lexical coverage (per dataset) ==\n",
      "data/clean/labeled-sentiment_2col.xlsx: W2V=0.932, FT(vocab)=0.932\n",
      "data/clean/test__1__2col.xlsx: W2V=0.987, FT(vocab)=0.987\n",
      "data/clean/train__3__2col.xlsx: W2V=0.990, FT(vocab)=0.990\n",
      "data/clean/train-00000-of-00001_2col.xlsx: W2V=0.943, FT(vocab)=0.943\n",
      "data/clean/merged_dataset_CSV__1__2col.xlsx: W2V=0.949, FT(vocab)=0.949\n"
     ]
    }
   ],
   "source": [
    "# Compare Word2Vec vs FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "\n",
    "w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "ft = FastText.load(\"embeddings/fasttext.model\")\n",
    "\n",
    "seed_words = [\"yaxşı\",\"pis\",\"çox\",\"bahalı\",\"ucuz\",\"mükəmməl\",\"dəhşət\",\n",
    "              \"<PRICE>\",\"<RATING_POS>\"]\n",
    "\n",
    "syn_pairs = [(\"yaxşı\",\"əla\"), (\"bahalı\",\"qiymətli\"), (\"ucuz\",\"sərfəli\")]\n",
    "ant_pairs = [(\"yaxşı\",\"pis\"), (\"bahalı\",\"ucuz\")]\n",
    "\n",
    "def read_tokens(f):\n",
    "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
    "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
    "\n",
    "def lexical_coverage(model, tokens):\n",
    "    vocab = model.wv.key_to_index\n",
    "    return sum(1 for t in tokens if t in vocab) / max(1, len(tokens))\n",
    "\n",
    "print(\"== Lexical coverage (per dataset) ==\")\n",
    "for f in files:\n",
    "    toks = read_tokens(f)\n",
    "    cov_w2v = lexical_coverage(w2v, toks)\n",
    "    cov_ftv = lexical_coverage(ft, toks)\n",
    "    print(f\"{f}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a711e4c-a583-474c-be75-9b5374e396fa",
   "metadata": {},
   "source": [
    "**Lexical Coverage** measures how many of the tokens in your dataset are included in model's vocabulary.  \n",
    "It actually says that how well the model knows your corpus words.  \n",
    "\n",
    " $ \\text{Coverage} = \\frac{\\text{Number of tokens in vocabulary}}{\\text{Total number of tokens in the dataset}} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f318d5-fed4-40f0-9582-62442b6db33e",
   "metadata": {},
   "source": [
    "When we compare **Lexical Coverage** of both models for each dataset, We observe that values are closed to %100 percent which is good. Also, they do not differ each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1daf950-1102-46a4-adc5-b651ac2f28bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Similarity ==\n",
      "Synonyms: W2V=0.355, FT=0.442\n",
      "Antonyms: W2V=0.349, FT=0.436\n",
      "Separation: W2V=0.007, FT=0.005\n",
      "\n",
      "== Nearest Neighbors ==\n",
      "  W2V NN for 'yaxşı': ['<RATING_POS>', 'iyi', 'yaxshi', 'yaxsı', 'awsome']\n",
      "  FT NN for 'yaxşı': ['yaxşıı', 'yaxşıkı', 'yaxşıca', 'yaxş', 'yaxşıya']\n",
      "  W2V NN for 'pis': ['vərdişlərə', 'lire', 'kardeşi', 'xalçalardan', 'günd']\n",
      "  FT NN for 'pis': ['piis', 'pi', 'pisdii', 'pixlr', 'pisi']\n",
      "  W2V NN for 'çox': ['bəyənilsin', 'çoox', 'çöx', 'gözəldir', 'əladir']\n",
      "  FT NN for 'çox': ['çoxçox', 'çoxx', 'çoxh', 'ço', 'çoh']\n",
      "  W2V NN for 'bahalı': ['yaxtaları', 'metallarla', 'villaları', 'radiusda', 'portretlerinə']\n",
      "  FT NN for 'bahalı': ['bahalıı', 'bahalısı', 'bahalıq', 'baharlı', 'pahalı']\n",
      "  W2V NN for 'ucuz': ['düzəltdirilib', 'baha', 'qiymete', 'keyfiyetli', 'sududu']\n",
      "  FT NN for 'ucuz': ['ucuzu', 'ucuza', 'ucuzdu', 'ucuzluğa', 'ucuzdur']\n",
      "  W2V NN for 'mükəmməl': ['möhtəşəmm', 'kəliməylə', 'yaradilanlarin', 'mukəmməl', 'möhdəşəm']\n",
      "  FT NN for 'mükəmməl': ['mükəmməll', 'mükəməl', 'mükəmməldi', 'mukəmməl', 'mükəmməlsiz']\n",
      "  W2V NN for 'dəhşət': ['xalçalardan', 'birda', 'ayranları', 'açdıq', 'təsirlidi']\n",
      "  FT NN for 'dəhşət': ['dəhşətdü', 'dəhşətə', 'dəhşətizm', 'dəhşəti', 'dəhşətdi']\n",
      "  W2V NN for '<PRICE>': []\n",
      "  FT NN for '<PRICE>': ['cokubse', 'reeceep', 'engiltdere', 'ıngiltere', 'flight']\n",
      "  W2V NN for '<RATING_POS>': ['deneyin', 'uygulama', 'süper', 'çook', 'iternetsiz']\n",
      "  FT NN for '<RATING_POS>': ['<RATING_NEG>', 'çookk', 'süperr', 'çokk', 'çook']\n"
     ]
    }
   ],
   "source": [
    "def cos(a,b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "def pair_sim(model, pairs):  # compute similarity for each pair\n",
    "    vals = []\n",
    "    for a,b in pairs:\n",
    "        try:\n",
    "            vals.append(model.wv.similarity(a,b))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sum(vals)/len(vals) if vals else float('nan')\n",
    "\n",
    "syn_w2v = pair_sim(w2v, syn_pairs)\n",
    "syn_ft = pair_sim(ft, syn_pairs)\n",
    "ant_w2v = pair_sim(w2v, ant_pairs)\n",
    "ant_ft = pair_sim(ft, ant_pairs)\n",
    "\n",
    "print(\"\\n== Similarity ==\")\n",
    "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
    "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
    "print(f\"Separation: W2V={syn_w2v - ant_w2v:.3f}, FT={syn_ft - ant_ft:.3f}\")\n",
    "\n",
    "def neighbors(model, word, k=5):\n",
    "    try:\n",
    "        return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "print(\"\\n== Nearest Neighbors ==\")\n",
    "\"\"\"\n",
    "seed_words = [\"yaxşı\",\"pis\",\"çox\",\"bahalı\",\"ucuz\",\"mükəmməl\",\"dəhşət\",\n",
    "              \"<PRICE>\",\"<RATING_POS>\"]\n",
    "\"\"\"\n",
    "for w in seed_words:\n",
    "    print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
    "    print(f\"  FT NN for '{w}':\", neighbors(ft, w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fe7de-83ec-45b0-80a4-509bc9f9f3bc",
   "metadata": {},
   "source": [
    "#### Synonym/Antonym Similarity\n",
    "Similarity in word embeddings measures how close 2 vectors are in meaning.  \n",
    "Mathematically, it is computed as cosine similarity\n",
    "\n",
    "$ \\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\\\|a\\\\| \\\\|b\\\\|} $.\n",
    "\n",
    "Range from +1 to -1:\n",
    "- +1 means very similar.\n",
    "- 0 means unrelated.\n",
    "- -1 means opposite directions.\n",
    "\n",
    "**Separation** measures how well the model distinguishes between similar and opposite words.\n",
    "$ \\text{Separation} = \\text{mean(similarity of synonyms)} - \\text{mean(similarity of antonyms)} $.\n",
    "\n",
    "If separation is large, we observe that model clearly understand that synonyms are similar than antonyms.  \n",
    "However if it is small model is not good at distinguishing them.\n",
    "\n",
    "When we look at the output, **FastText** is better for both synonym and anthonym similarity. Separation result is small for both models-meaning they do not strongly separate meanings. \n",
    "\n",
    "**NOTE:** Limited corpus size or insufficient domain balance can cause bad results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8f978-8e31-4a63-8bad-1bbf1cffebe7",
   "metadata": {},
   "source": [
    "#### Nearest-neighbor quality.\n",
    "The **Nearest-Neighbor (NN)** metric means the words closest to a given word's vector. It is calculated by `Cosine Similarity`.\n",
    "\n",
    "When we look at the output, `FastText` have better results compared to `Word2Vec`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
