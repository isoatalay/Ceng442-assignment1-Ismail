{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d668edb0-7d1f-4bb4-80a2-3d5a5d33c0c5",
   "metadata": {},
   "source": [
    "`re` module let us do pattern matching operations using regular expressions.  \n",
    "`compile()` converts strings into **compiled regex object** which has methods like `.search()`, `.findall()`, `.match()` etc.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d3e01b-fe9a-4160-a00b-79a5bb924410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #regular expression\n",
    "\n",
    "# re.I -> flag for cas-insensitive matching (\"trend\" = \"Trend\")\n",
    "# (alternative1 | alternative2 | ......)\n",
    "NEWS_HINTS = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
    "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)\")\n",
    "REV_HINTS = re.compile(r\"\\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\\b\", re.I)\n",
    "\n",
    "def detect_domain(text: str) -> str:\n",
    "    s = text.lower()\n",
    "    if NEWS_HINTS.search(s):\n",
    "        return \"news\"\n",
    "    if SOCIAL_HINTS.search(s):\n",
    "        return \"social\"\n",
    "    if REV_HINTS.search(s):\n",
    "        return \"reviews\"\n",
    "    return \"general\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d447a86-6b58-4b1f-96cf-8128ad2378c5",
   "metadata": {},
   "source": [
    "So we have normally 4 different domains which are `reviews`, `social`, `news`, `general`.  \n",
    "On the following cell, we detect **review based expressions** in Azerbaijani text (e.g. 20 manat, 5 ulduz). Later, models such as Word2Vec, FastText can learn meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c637db3-aa8d-4cd8-b459-1e5aa4b18d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Domain-specific normalization (reviews) ---\n",
    "\n",
    "PRICE_RE = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
    "STARS_RE = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
    "POS_RATE = re.compile(r\"\\b√ßox yax≈üƒ±\\b\")\n",
    "NEG_RATE = re.compile(r\"\\b√ßox pis\\b\")\n",
    "\n",
    "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
    "    if domain == \"reviews\":\n",
    "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
    "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", s)\n",
    "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
    "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
    "        return \" \".join(s.split())\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688eec5-414e-4eb3-a0ff-e63e20b94c9e",
   "metadata": {},
   "source": [
    "Domain tag is a prefix that is added tƒ± text lines to help models distinguish between different types of text.  \n",
    "(Models can be `Word2Vec`, `FastText`, `BERT`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6682faf-d05e-4799-8c96-f2fc1f9a4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Domain tag token for corpus \"domreview + data\"\n",
    "def add_domain_tag(line: str, domain: str) -> str:\n",
    "    return f\"dom{domain} \" + line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a678ce0-8cc0-4ca1-a304-0f30a4912722",
   "metadata": {},
   "source": [
    "This following cell handles: \n",
    "- **Encoding problems**  (l‚Äôhumanit√É¬© ‚Üí l'humanit√©)\n",
    "- **Punctuations**  (https://... ‚Üí `URL`)\n",
    "- **URLs** (????? ‚Üí ?) \n",
    "- **Emojis**  (üòä ‚Üí `EMO_POS`)\n",
    "- **ƒ∞nformal writing**  (slm ‚Üí salam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b98f4aed-e3a1-42fb-a90f-dfebe41eac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import html, unicodedata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ftfy \"fixes text for you\" cleans encoding problems \"l‚Äôhumanit√É¬©\"  --> \"l'humanit√©\"\n",
    "try:\n",
    "    from ftfy import fix_text\n",
    "except Exception:\n",
    "    def fix_text(s): return s\n",
    "\n",
    "# Azerbaijani-aware lowercase\n",
    "def lower_az(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = s.replace(\"I\", \"ƒ±\").replace(\"ƒ∞\", \"i\") #Azerbaijani casing rules\n",
    "    s = s.lower().replace(\"iÃá\", \"i\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# These define patterns to detect unwanted elements\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
    "USER_RE = re.compile(r\"@\\w+\")\n",
    "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
    "MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "REPEAT_CHARS = re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE) # cooool -> cool\n",
    "\n",
    "# TOKEN_RE defines what a valid token looks like\n",
    "TOKEN_RE = re.compile(\n",
    "    r\"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?\"\n",
    "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO_(?:POS|NEG)\"\n",
    ")\n",
    "\n",
    "# Before tokenization, we replaces emojis with two tags.\n",
    "# This preserves sentiment signal even we later strip punctuations.\n",
    "EMO_MAP = {\n",
    "    \"üôÇ\": \"EMO_POS\", \"üòÄ\": \"EMO_POS\", \"üòç\": \"EMO_POS\", \"üòä\": \"EMO_POS\" ,\"üëç\": \"EMO_POS\",\n",
    "    \"‚òπ\": \"EMO_NEG\", \"üôÅ\": \"EMO_NEG\", \"üò†\": \"EMO_NEG\", \"üò°\": \"EMO_NEG\", \"üëé\": \"EMO_NEG\"\n",
    "}\n",
    "\n",
    "# slang map to standardize common informal forms\n",
    "SLANG_MAP = {\"slm\": \"salam\", \"tmm\": \"tamam\", \"sagol\": \"saƒüol\", \"cox\": \"√ßox\", \"yaxsi\": \"yax≈üƒ±\"}\n",
    "NEGATORS = {\"yox\", \"deyil\", \"he√ß\", \"q…ôtiyy…ôn\", \"yoxdur\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7cf12-fe91-43bd-9279-6bd691e89ef9",
   "metadata": {},
   "source": [
    "On the below code snippet, we clean and tokenize the text with what we define above. (`regex patterns`, `.lower_az() function`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e3d511d-b71b-42ff-bb31-7b88fba8cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "\n",
    "    # emoji map\n",
    "    for emo, tag in EMO_MAP.items():\n",
    "        s = s.replace(emo, f\" {tag} \")  # Converting emojis to emo_tags\n",
    "\n",
    "    s = fix_text(s)  # clean encoding problems\n",
    "    s = html.unescape(s)  # decoces html entities \n",
    "    s = HTML_TAG_RE.sub(\" \", s)  # strip any <tag> markup\n",
    "    s = URL_RE.sub(\" URL \", s)  # Replaces linkswith URL, Email, Phone patterns\n",
    "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
    "    s = PHONE_RE.sub(\" PHONE \", s)\n",
    "\n",
    "    # Remove the # hashtag symbol but keep the inner text\n",
    "    # if the inner text is written camelCase, insert space\n",
    "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" +\n",
    "               re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \", s)\n",
    "\n",
    "    # convert @name to USER\n",
    "    s = USER_RE.sub(\" USER \", s)\n",
    "    s = lower_az(s)  # Azerbaijani-aware lowercasing\n",
    "\n",
    "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
    "\n",
    "    if numbers_to_token:\n",
    "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
    "\n",
    "    if keep_sentence_punct:\n",
    "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ.!?]\", \" \", s)\n",
    "    else:\n",
    "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", s)\n",
    "\n",
    "    s = MULTI_SPACE.sub(\" \", s).strip()  # Remove if more than 1 space\n",
    "    toks = TOKEN_RE.findall(s)  # Finds all regex patterns\n",
    "\n",
    "    norm = [] # \n",
    "    mark_neg = 0\n",
    "    for t in toks:\n",
    "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
    "        t = SLANG_MAP.get(t, t)\n",
    "\n",
    "        if t in NEGATORS:\n",
    "            norm.append(t)\n",
    "            mark_neg = 3\n",
    "            continue\n",
    "\n",
    "        if mark_neg > 0 and t not in {\"URL\", \"EMAIL\", \"PHONE\", \"USER\"}:\n",
    "            norm.append(t + \"_NEG\")\n",
    "            mark_neg -= 1 \n",
    "        else:\n",
    "            norm.append(t)\n",
    "\n",
    "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\", \"e\"})]\n",
    "    return \" \".join(norm).strip()  # Remove single character tokens except \"o\", \"e\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843919b-5ca5-4c25-8201-78c5338f7f5d",
   "metadata": {},
   "source": [
    "This function standardizes labels from different datasets into a uniform numeric sentiment value for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47009e54-9239-4a9c-8403-78c78794237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment_value(v, scheme: str):  # v : raw sentiment label\n",
    "    if scheme == \"binary\":\n",
    "        try:\n",
    "            return 1.0 if int(v) == 1 else 0.0\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    s = str(v).strip().lower() \n",
    "    if s in {\"pos\",\"positive\",\"1\",\"m√ºsb…ôt\",\"good\",\"pozitiv\"}:\n",
    "        return 1.0\n",
    "    if s in {\"neu\",\"neutral\",\"2\",\"neytral\"}:\n",
    "        return 0.5\n",
    "    if s in {\"neg\",\"negative\",\"0\",\"m…ônfi\",\"bad\",\"neqativ\"}:\n",
    "        return 0.0\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cad80-157f-4143-af99-0f3fedc21d07",
   "metadata": {},
   "source": [
    "##### These following code snippet captures many processes above. \n",
    "- `Reading` pd.read_excel()\n",
    "- `Cleaning` dropna() , dropduplicates()\n",
    "- `Normalization` normalize_text_az()\n",
    "- `Domain Detection` detect_domain()\n",
    "- `Label Mapping` map_sentiment_value()\n",
    "- `Exporting` out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b55b1acd-f41e-4313-9b0b-8cbea578750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):\n",
    "    df = pd.read_excel(in_path)\n",
    "\n",
    "    for c in [\"Unnamed: 0\", \"index\"]:  # Remove useless columns\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    assert text_col in df.columns and label_col in df.columns  # Check needed columns exist\n",
    "\n",
    "    df = df.dropna(subset=[text_col])  # Remove null cells\n",
    "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
    "    df = df.drop_duplicates(subset=[text_col])  # Remove duplicates\n",
    "\n",
    "    # Call the .normalize_text_az() method \n",
    "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s)) \n",
    "    # Call the detect_domain()\n",
    "    df[\"domain\"] = df[text_col].astype(str).apply(detect_domain)\n",
    "    # Call the domain_specific_normalize for review \n",
    "    df[\"cleaned_text\"] = df.apply(\n",
    "        lambda r: domain_specific_normalize(r[\"cleaned_text\"], r[\"domain\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Stopword are useless words for model learning.\n",
    "    # Also it counts too much which causes use of large memory\n",
    "    if remove_stopwords:\n",
    "        sw = set([\"v…ô\",\"il…ô\",\"amma\",\"ancaq\",\"lakin\",\"ya\",\"h…ôm\",\"ki\",\"bu\",\"bir\",\n",
    "                  \"o\",\"biz\",\"siz\",\"m…ôn\",\"s…ôn\",\"orada\",\"burada\",\"b√ºt√ºn\",\n",
    "                  \"h…ôr\",\"artƒ±q\",\"√ßox\",\"az\",\"…ôn\",\"d…ô\",\"da\",\"√º√ß√ºn\"])\n",
    "        for keep in [\"deyil\",\"yox\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"]:\n",
    "            sw.discard(keep)\n",
    "\n",
    "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(\n",
    "            lambda s: \" \".join([t for t in s.split() if t not in sw])\n",
    "        )\n",
    "\n",
    "    # Calling .map_sentiment_value() method to specify sentiment value\n",
    "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
    "    df = df.dropna(subset=[\"sentiment_value\"])  # Drop null sentiment values\n",
    "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
    "\n",
    "    # we have two columns: \"cleaned_text\", \"sentiment_value\"\n",
    "    # These files will be needed fƒ±r corpus file and embedding process.\n",
    "    out_df = df[[\"cleaned_text\", \"sentiment_value\"]].reset_index(drop=True)\n",
    "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_df.to_excel(out_two_col_path, index=False)\n",
    "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa37d48-cf85-4819-8919-6d609ac71f3d",
   "metadata": {},
   "source": [
    "##### These following method merges all excel datasets into txt file where each line:\n",
    "- One Sentence\n",
    "- Starts with domain tag\n",
    "- lowercased, punc-free and ready for Word2Vec /Faxtext training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f653b64a-21b2-4a58-8040-3140e2d8e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
    "    lines = []\n",
    "    for (f, text_col) in zip(input_files, text_cols):\n",
    "        df = pd.read_excel(f)\n",
    "        for raw in df[text_col].dropna().astype(str):\n",
    "            dom = detect_domain(raw)\n",
    "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
    "            parts = re.split(r\"[.!?]+\", s)\n",
    "            for p in parts:\n",
    "                p = p.strip()\n",
    "                if not p:\n",
    "                    continue\n",
    "                p = re.sub(r\"[^\\w\\s…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", p)\n",
    "                p = \" \".join(p.split()).lower()\n",
    "                if p:\n",
    "                    lines.append(f\"dom{dom} \" + p)\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
    "        for ln in lines:\n",
    "            w.write(ln + \"\\n\")\n",
    "    print(f\"Wrote {out_txt} with {len(lines)} lines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965d358-30ff-438f-b471-2e5f04051d1c",
   "metadata": {},
   "source": [
    "Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39bc9484-1d41-4667-8ad6-f5b108832098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\clean\\labeled-sentiment_2col.xlsx (rows=2955)\n",
      "Saved: data\\clean\\test__1__2col.xlsx (rows=4198)\n",
      "Saved: data\\clean\\train__3__2col.xlsx (rows=19557)\n",
      "Saved: data\\clean\\train-00000-of-00001_2col.xlsx (rows=41756)\n",
      "Saved: data\\clean\\merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
      "Wrote data\\clean\\corpus_all.txt with 124353 lines\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "if __name__ == \"__main__\":\n",
    "    RAW_DIR   = Path(\"data/raw\")    # giri≈ü dosyalarƒ± (ham/i≈ülenmemi≈ü)\n",
    "    CLEAN_DIR = Path(\"data/clean\")  # √ßƒ±kƒ±≈ü dosyalarƒ± (iki kolon + corpus)\n",
    "    CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    CFG = [\n",
    "        (RAW_DIR / \"labeled-sentiment.xlsx\", \"text\", \"sentiment\", \"tri\"),\n",
    "        (RAW_DIR / \"test__1_.xlsx\", \"text\", \"label\", \"binary\"),\n",
    "        (RAW_DIR / \"train__3_.xlsx\", \"text\", \"label\", \"binary\"),\n",
    "        (RAW_DIR / \"train-00000-of-00001.xlsx\", \"text\", \"labels\", \"tri\"),\n",
    "        (RAW_DIR / \"merged_dataset_CSV__1_.xlsx\", \"text\", \"labels\", \"binary\"),\n",
    "    ]\n",
    "\n",
    "    for fname, tcol, lcol, scheme in CFG:\n",
    "        out = CLEAN_DIR / f\"{Path(fname).stem}_2col.xlsx\"\n",
    "        process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)\n",
    "\n",
    "    corpus_path = CLEAN_DIR / \"corpus_all.txt\"\n",
    "    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG], out_txt=str(corpus_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
