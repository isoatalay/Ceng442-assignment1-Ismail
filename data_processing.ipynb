{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d668edb0-7d1f-4bb4-80a2-3d5a5d33c0c5",
   "metadata": {},
   "source": [
    "`re` module let us do pattern matching operations using regular expressions.  \n",
    "`compile()` converts strings into **compiled regex object** which has methods like `.search()`, `.findall()`, `.match()` etc.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d3e01b-fe9a-4160-a00b-79a5bb924410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #regular expression\n",
    "\n",
    "# re.I -> flag for cas-insensitive matching (\"trend\" = \"Trend\")\n",
    "# (alternative1 | alternative2 | ......)\n",
    "NEWS_HINTS = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
    "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)\")\n",
    "REV_HINTS = re.compile(r\"\\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\\b\", re.I)\n",
    "\n",
    "def detect_domain(text: str) -> str:\n",
    "    s = text.lower()\n",
    "    if NEWS_HINTS.search(s):\n",
    "        return \"news\"\n",
    "    if SOCIAL_HINTS.search(s):\n",
    "        return \"social\"\n",
    "    if REV_HINTS.search(s):\n",
    "        return \"reviews\"\n",
    "    return \"general\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d447a86-6b58-4b1f-96cf-8128ad2378c5",
   "metadata": {},
   "source": [
    "So we have normally 4 different domains which are `reviews`, `social`, `news`, `general`.  \n",
    "On the following cell, we detect **review based expressions** in Azerbaijani text (e.g. 20 manat, 5 ulduz). Later, models such as Word2Vec, FastText can learn meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c637db3-aa8d-4cd8-b459-1e5aa4b18d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Domain-specific normalization (reviews) ---\n",
    "\n",
    "PRICE_RE = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
    "STARS_RE = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
    "POS_RATE = re.compile(r\"\\b√ßox yax≈üƒ±\\b\")\n",
    "NEG_RATE = re.compile(r\"\\b√ßox pis\\b\")\n",
    "\n",
    "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
    "    if domain == \"reviews\":\n",
    "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
    "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", s)\n",
    "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
    "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
    "        return \" \".join(s.split())\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688eec5-414e-4eb3-a0ff-e63e20b94c9e",
   "metadata": {},
   "source": [
    "Domain tag is a prefix that is added tƒ± text lines to help models distinguish between different types of text.  \n",
    "(Models can be `Word2Vec`, `FastText`, `BERT`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6682faf-d05e-4799-8c96-f2fc1f9a4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Domain tag token for corpus \"domreview + data\"\n",
    "def add_domain_tag(line: str, domain: str) -> str:\n",
    "    return f\"dom{domain} \" + line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a678ce0-8cc0-4ca1-a304-0f30a4912722",
   "metadata": {},
   "source": [
    "This following cell handles: \n",
    "- **Encoding problems**  (l‚Äôhumanit√É¬© ‚Üí l'humanit√©)\n",
    "- **Punctuations**  (https://... ‚Üí `URL`)\n",
    "- **URLs** (????? ‚Üí ?) \n",
    "- **Emojis**  (üòä ‚Üí `EMO_POS`)\n",
    "- **ƒ∞nformal writing**  (slm ‚Üí salam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98f4aed-e3a1-42fb-a90f-dfebe41eac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import html, unicodedata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ftfy \"fixes text for you\" cleans encoding problems \"l‚Äôhumanit√É¬©\"  --> \"l'humanit√©\"\n",
    "try:\n",
    "    from ftfy import fix_text\n",
    "except Exception:\n",
    "    def fix_text(s): return s\n",
    "\n",
    "# Azerbaijani-aware lowercase\n",
    "def lower_az(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = s.replace(\"I\", \"ƒ±\").replace(\"ƒ∞\", \"i\") #Azerbaijani casing rules\n",
    "    s = s.lower().replace(\"iÃá\", \"i\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# These define patterns to detect unwanted elements\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
    "USER_RE = re.compile(r\"@\\w+\")\n",
    "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
    "MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "REPEAT_CHARS = re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE) # cooool -> cool\n",
    "\n",
    "# TOKEN_RE defines what a valid token looks like\n",
    "TOKEN_RE = re.compile(\n",
    "    r\"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?\"\n",
    "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO_(?:POS|NEG)\"\n",
    ")\n",
    "\n",
    "# Before tokenization, we replaces emojis with two tags.\n",
    "# This preserves sentiment signal even we later strip punctuations.\n",
    "EMO_MAP = {\n",
    "    \"üôÇ\": \"EMO_POS\", \"üòÄ\": \"EMO_POS\", \"üòç\": \"EMO_POS\", \"üòä\": \"EMO_POS\" ,\"üëç\": \"EMO_POS\",\n",
    "    \"‚òπ\": \"EMO_NEG\", \"üôÅ\": \"EMO_NEG\", \"üò†\": \"EMO_NEG\", \"üò°\": \"EMO_NEG\", \"üëé\": \"EMO_NEG\"\n",
    "}\n",
    "\n",
    "# slang map to standardize common informal forms\n",
    "SLANG_MAP = {\"slm\": \"salam\", \"tmm\": \"tamam\", \"sagol\": \"saƒüol\", \"cox\": \"√ßox\", \"yaxsi\": \"yax≈üƒ±\"}\n",
    "NEGATORS = {\"yox\", \"deyil\", \"he√ß\", \"q…ôtiyy…ôn\", \"yoxdur\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7cf12-fe91-43bd-9279-6bd691e89ef9",
   "metadata": {},
   "source": [
    "On the below code snippet, we clean and tokenize the text with what we define above. (`regex patterns`, `.lower_az() function`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3d511d-b71b-42ff-bb31-7b88fba8cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "\n",
    "    # emoji map\n",
    "    for emo, tag in EMO_MAP.items():\n",
    "        s = s.replace(emo, f\" {tag} \")  # Converting emojis to emo_tags\n",
    "\n",
    "    s = fix_text(s)  # clean encoding problems\n",
    "    s = html.unescape(s)  # decoces html entities \n",
    "    s = HTML_TAG_RE.sub(\" \", s)  # strip any <tag> markup\n",
    "    s = URL_RE.sub(\" URL \", s)  # Replaces linkswith URL, Email, Phone patterns\n",
    "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
    "    s = PHONE_RE.sub(\" PHONE \", s)\n",
    "\n",
    "    # Remove the # hashtag symbol but keep the inner text\n",
    "    # if the inner text is written camelCase, insert space\n",
    "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" +\n",
    "               re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \", s)\n",
    "\n",
    "    # convert @name to USER\n",
    "    s = USER_RE.sub(\" USER \", s)\n",
    "    s = lower_az(s)  # Azerbaijani-aware lowercasing\n",
    "\n",
    "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
    "\n",
    "    if numbers_to_token:\n",
    "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
    "\n",
    "    if keep_sentence_punct:\n",
    "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ.!?]\", \" \", s)\n",
    "    else:\n",
    "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", s)\n",
    "\n",
    "    s = MULTI_SPACE.sub(\" \", s).strip()  # Remove if more than 1 space\n",
    "    toks = TOKEN_RE.findall(s)  # Finds all regex patterns\n",
    "\n",
    "    norm = [] # \n",
    "    mark_neg = 0\n",
    "    for t in toks:\n",
    "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
    "        t = SLANG_MAP.get(t, t)\n",
    "\n",
    "        if t in NEGATORS:\n",
    "            norm.append(t)\n",
    "            mark_neg = 3\n",
    "            continue\n",
    "\n",
    "        if mark_neg > 0 and t not in {\"URL\", \"EMAIL\", \"PHONE\", \"USER\"}:\n",
    "            norm.append(t + \"_NEG\")\n",
    "            mark_neg -= 1 \n",
    "        else:\n",
    "            norm.append(t)\n",
    "\n",
    "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\", \"e\"})]\n",
    "    return \" \".join(norm).strip()  # Remove single character tokens except \"o\", \"e\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843919b-5ca5-4c25-8201-78c5338f7f5d",
   "metadata": {},
   "source": [
    "This function standardizes labels from different datasets into a uniform numeric sentiment value for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47009e54-9239-4a9c-8403-78c78794237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment_value(v, scheme: str):  # v : raw sentiment label\n",
    "    if scheme == \"binary\":\n",
    "        try:\n",
    "            return 1.0 if int(v) == 1 else 0.0\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    s = str(v).strip().lower() \n",
    "    if s in {\"pos\",\"positive\",\"1\",\"m√ºsb…ôt\",\"good\",\"pozitiv\"}:\n",
    "        return 1.0\n",
    "    if s in {\"neu\",\"neutral\",\"2\",\"neytral\"}:\n",
    "        return 0.5\n",
    "    if s in {\"neg\",\"negative\",\"0\",\"m…ônfi\",\"bad\",\"neqativ\"}:\n",
    "        return 0.0\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cad80-157f-4143-af99-0f3fedc21d07",
   "metadata": {},
   "source": [
    "##### These following code snippet captures many processes above. \n",
    "- `Reading` pd.read_excel()\n",
    "- `Cleaning` dropna() , dropduplicates()\n",
    "- `Normalization` normalize_text_az()\n",
    "- `Domain Detection` detect_domain()\n",
    "- `Label Mapping` map_sentiment_value()\n",
    "- `Exporting` out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55b1acd-f41e-4313-9b0b-8cbea578750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):\n",
    "    df = pd.read_excel(in_path)\n",
    "\n",
    "    for c in [\"Unnamed: 0\", \"index\"]:  # Remove useless columns\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    assert text_col in df.columns and label_col in df.columns  # Check needed columns exist\n",
    "\n",
    "    df = df.dropna(subset=[text_col])  # Remove null cells\n",
    "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
    "    df = df.drop_duplicates(subset=[text_col])  # Remove duplicates\n",
    "\n",
    "    # Call the .normalize_text_az() method \n",
    "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s)) \n",
    "    # Call the detect_domain()\n",
    "    df[\"domain\"] = df[text_col].astype(str).apply(detect_domain)\n",
    "    # Call the domain_specific_normalize for review \n",
    "    df[\"cleaned_text\"] = df.apply(\n",
    "        lambda r: domain_specific_normalize(r[\"cleaned_text\"], r[\"domain\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Stopword are useless words for model learning.\n",
    "    # Also it counts too much which causes use of large memory\n",
    "    if remove_stopwords:\n",
    "        sw = set([\"v…ô\",\"il…ô\",\"amma\",\"ancaq\",\"lakin\",\"ya\",\"h…ôm\",\"ki\",\"bu\",\"bir\",\n",
    "                  \"o\",\"biz\",\"siz\",\"m…ôn\",\"s…ôn\",\"orada\",\"burada\",\"b√ºt√ºn\",\n",
    "                  \"h…ôr\",\"artƒ±q\",\"√ßox\",\"az\",\"…ôn\",\"d…ô\",\"da\",\"√º√ß√ºn\"])\n",
    "        for keep in [\"deyil\",\"yox\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"]:\n",
    "            sw.discard(keep)\n",
    "\n",
    "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(\n",
    "            lambda s: \" \".join([t for t in s.split() if t not in sw])\n",
    "        )\n",
    "\n",
    "    # Calling .map_sentiment_value() method to specify sentiment value\n",
    "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
    "    df = df.dropna(subset=[\"sentiment_value\"])  # Drop null sentiment values\n",
    "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
    "\n",
    "    # we have two columns: \"cleaned_text\", \"sentiment_value\"\n",
    "    # These files will be needed fƒ±r corpus file and embedding process.\n",
    "    out_df = df[[\"cleaned_text\", \"sentiment_value\"]].reset_index(drop=True)\n",
    "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_df.to_excel(out_two_col_path, index=False)\n",
    "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa37d48-cf85-4819-8919-6d609ac71f3d",
   "metadata": {},
   "source": [
    "##### These following method merges all excel datasets into txt file where each line:\n",
    "- One Sentence\n",
    "- Starts with domain tag\n",
    "- lowercased, punc-free and ready for Word2Vec /Faxtext training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f653b64a-21b2-4a58-8040-3140e2d8e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
    "    lines = []\n",
    "    for (f, text_col) in zip(input_files, text_cols):\n",
    "        df = pd.read_excel(f)\n",
    "        for raw in df[text_col].dropna().astype(str):\n",
    "            dom = detect_domain(raw)\n",
    "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
    "            parts = re.split(r\"[.!?]+\", s)\n",
    "            for p in parts:\n",
    "                p = p.strip()\n",
    "                if not p:\n",
    "                    continue\n",
    "                p = re.sub(r\"[^\\w\\s…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", p)\n",
    "                p = \" \".join(p.split()).lower()\n",
    "                if p:\n",
    "                    lines.append(f\"dom{dom} \" + p)\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
    "        for ln in lines:\n",
    "            w.write(ln + \"\\n\")\n",
    "    print(f\"Wrote {out_txt} with {len(lines)} lines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965d358-30ff-438f-b471-2e5f04051d1c",
   "metadata": {},
   "source": [
    "Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39bc9484-1d41-4667-8ad6-f5b108832098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: labeled-sentiment_2col.xlsx (rows=2955)\n",
      "Saved: test__1__2col.xlsx (rows=4198)\n",
      "Saved: train__3__2col.xlsx (rows=19557)\n",
      "Saved: train-00000-of-00001_2col.xlsx (rows=41756)\n",
      "Saved: merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
      "Wrote corpus_all.txt with 124353 lines\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CFG = [\n",
    "        (\"labeled-sentiment.xlsx\", \"text\", \"sentiment\", \"tri\"),\n",
    "        (\"test__1_.xlsx\", \"text\", \"label\", \"binary\"),\n",
    "        (\"train__3_.xlsx\", \"text\", \"label\", \"binary\"),\n",
    "        (\"train-00000-of-00001.xlsx\", \"text\", \"labels\", \"tri\"),\n",
    "        (\"merged_dataset_CSV__1_.xlsx\", \"text\", \"labels\", \"binary\"),\n",
    "    ]\n",
    "\n",
    "    for fname, tcol, lcol, scheme in CFG:\n",
    "        out = f\"{Path(fname).stem}_2col.xlsx\"\n",
    "        process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)\n",
    "\n",
    "    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG], out_txt=\"corpus_all.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171cb33",
   "metadata": {},
   "source": [
    "After cleaning, normalizing and labeling data, they are ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a84c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec & FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# The cleaned, normalized, labeled data files that are processed in \"data_prcocessing.ipynb\" \n",
    "files = [\n",
    "    \"labeled-sentiment_2col.xlsx\",\n",
    "    \"test__1__2col.xlsx\",\n",
    "    \"train__3__2col.xlsx\",\n",
    "    \"train-00000-of-00001_2col.xlsx\",\n",
    "    \"merged_dataset_CSV__1__2col.xlsx\",\n",
    "]\n",
    "\n",
    "sentences = []\n",
    "for f in files:\n",
    "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
    "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
    "\n",
    "Path(\"embeddings\").mkdir(exist_ok=True)  # Create a folder named embeddings\n",
    "\n",
    "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5,\n",
    "               min_count=3, sg=1, negative=10, epochs=10)  # Train Word2Vec\n",
    "w2v.save(\"embeddings/word2vec.model\")\n",
    "\n",
    "print(\"Saved Word2Vec model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd381e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FaxtText model\n"
     ]
    }
   ],
   "source": [
    "ft = FastText(sentences=sentences, vector_size=300, window=5,\n",
    "              min_count=3, sg=1, min_n=3, max_n=6, epochs=10)  # Train FastText\n",
    "ft.save(\"embeddings/fasttext.model\")\n",
    "\n",
    "print(\"Saved FaxtText model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced1fe1",
   "metadata": {},
   "source": [
    "Now, we will compare **Word2Vec** vs **FastText** using metrics like:  \n",
    "- Coverage  \n",
    "- Synonym/Antonym similarity\n",
    "- Nearest-neighbor quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff0a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Lexical coverage (per dataset) ==\n",
      "labeled-sentiment_2col.xlsx: W2V=0.932, FT(vocab)=0.932\n",
      "test__1__2col.xlsx: W2V=0.987, FT(vocab)=0.987\n",
      "train__3__2col.xlsx: W2V=0.990, FT(vocab)=0.990\n",
      "train-00000-of-00001_2col.xlsx: W2V=0.943, FT(vocab)=0.943\n",
      "merged_dataset_CSV__1__2col.xlsx: W2V=0.949, FT(vocab)=0.949\n"
     ]
    }
   ],
   "source": [
    "# Compare Word2Vec vs FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "\n",
    "w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "ft = FastText.load(\"embeddings/fasttext.model\")\n",
    "\n",
    "seed_words = [\"yax≈üƒ±\",\"pis\",\"√ßox\",\"bahalƒ±\",\"ucuz\",\"m√ºk…ômm…ôl\",\"d…ôh≈ü…ôt\",\n",
    "              \"<PRICE>\",\"<RATING_POS>\"]\n",
    "\n",
    "syn_pairs = [(\"yax≈üƒ±\",\"…ôla\"), (\"bahalƒ±\",\"qiym…ôtli\"), (\"ucuz\",\"s…ôrf…ôli\")]\n",
    "ant_pairs = [(\"yax≈üƒ±\",\"pis\"), (\"bahalƒ±\",\"ucuz\")]\n",
    "\n",
    "def read_tokens(f):\n",
    "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
    "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
    "\n",
    "def lexical_coverage(model, tokens):\n",
    "    vocab = model.wv.key_to_index\n",
    "    return sum(1 for t in tokens if t in vocab) / max(1, len(tokens))\n",
    "\n",
    "print(\"== Lexical coverage (per dataset) ==\")\n",
    "for f in files:\n",
    "    toks = read_tokens(f)\n",
    "    cov_w2v = lexical_coverage(w2v, toks)\n",
    "    cov_ftv = lexical_coverage(ft, toks)\n",
    "    print(f\"{f}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a633b09",
   "metadata": {},
   "source": [
    "**Lexical Coverage** measures how many of the tokens in your dataset are included in model's vocabulary.  \n",
    "It actually says that how well the model knows your corpus words.  \n",
    "\n",
    " $ \\text{Coverage} = \\frac{\\text{Number of tokens in vocabulary}}{\\text{Total number of tokens in the dataset}} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944018e",
   "metadata": {},
   "source": [
    "When we compare **Lexical Coverage** of both models for each dataset, We observe that values are closed to %100 percent which is good. Also, they do not differ each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cda6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Similarity ==\n",
      "Synonyms: W2V=0.356, FT=0.435\n",
      "Antonyms: W2V=0.343, FT=0.435\n",
      "Separation: W2V=0.013, FT=0.001\n",
      "\n",
      "== Nearest Neighbors ==\n",
      "  W2V NN for 'yax≈üƒ±': ['iyi', '<RATING_POS>', 'yaxshi', 'yaxwi', 'yaxsƒ±']\n",
      "  FT NN for 'yax≈üƒ±': ['yax≈üƒ±ƒ±', 'yax≈üƒ±kƒ±', 'yax≈üƒ±ca', 'yax≈ü', 'yax≈üƒ±ya']\n",
      "  W2V NN for 'pis': ['g√ºnd', '<RATING_NEG>', 'v…ôrdi≈ül…ôr…ô', 'bugunki', 'sport']\n",
      "  FT NN for 'pis': ['piis', 'pi', 'pisdii', 'pisi', 'pis…ô']\n",
      "  W2V NN for '√ßox': ['√ß√∂x', '√ßoox', 'b…ôy…ônilsin', 'g√∂z…ôldir', 'cooxx']\n",
      "  FT NN for '√ßox': ['√ßox√ßox', '√ßoxx', '√ßoxh', '√ßo', '√ßoh']\n",
      "  W2V NN for 'bahalƒ±': ['yaxtalarƒ±', 'metallarla', 'radiusda', 'qabardƒ±lƒ±r', 'portretlerin…ô']\n",
      "  FT NN for 'bahalƒ±': ['bahalƒ±ƒ±', 'bahalƒ±sƒ±', 'bahalƒ±q', 'baharlƒ±', 'bahalƒ±ƒüƒ±']\n",
      "  W2V NN for 'ucuz': ['≈üeytanbazardan', 'd√ºz…ôltdirilib', 'sududu', 'qiymete', 'sorbasi']\n",
      "  FT NN for 'ucuz': ['ucuzu', 'ucuza', 'ucuzdu', 'ucuzluƒüa', 'ucuzdur']\n",
      "  W2V NN for 'm√ºk…ômm…ôl': ['m√∂ht…ô≈ü…ômm', 'k…ôlim…ôyl…ô', 'muk…ômm…ôl', 'm√∂hd…ô≈ü…ôm', 'bayƒ±ldƒ±m']\n",
      "  FT NN for 'm√ºk…ômm…ôl': ['m√ºk…ômm…ôll', 'm√ºk…ôm…ôl', 'm√ºk…ômm…ôldi', 'muk…ômm…ôl', 'm√ºk…ômm…ôlsiz']\n",
      "  W2V NN for 'd…ôh≈ü…ôt': ['xal√ßalardan', 'birda', 'soundtreki', 't…ôsirlidi', 'a√ßdƒ±q']\n",
      "  FT NN for 'd…ôh≈ü…ôt': ['d…ôh≈ü…ôtd√º', 'd…ôh≈ü…ôt…ô', 'd…ôh≈ü…ôti', 'd…ôh≈ü…ôtizm', 'd…ôh≈ü…ôttli']\n",
      "  W2V NN for '<PRICE>': []\n",
      "  FT NN for '<PRICE>': ['reeceep', 'cruise', 'recebzade', 'cokubse', 'lifcikle']\n",
      "  W2V NN for '<RATING_POS>': ['uygulama', 'deneyin', 'bence', 'internetli', 's√ºper']\n",
      "  FT NN for '<RATING_POS>': ['<RATING_NEG>', 's√ºperr', 's√ºper', '√ßookk', 'c√∂x']\n"
     ]
    }
   ],
   "source": [
    "def cos(a,b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "def pair_sim(model, pairs):  # compute similarity for each pair\n",
    "    vals = []\n",
    "    for a,b in pairs:\n",
    "        try:\n",
    "            vals.append(model.wv.similarity(a,b))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sum(vals)/len(vals) if vals else float('nan')\n",
    "\n",
    "syn_w2v = pair_sim(w2v, syn_pairs)\n",
    "syn_ft = pair_sim(ft, syn_pairs)\n",
    "ant_w2v = pair_sim(w2v, ant_pairs)\n",
    "ant_ft = pair_sim(ft, ant_pairs)\n",
    "\n",
    "print(\"\\n== Similarity ==\")\n",
    "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
    "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
    "print(f\"Separation: W2V={syn_w2v - ant_w2v:.3f}, FT={syn_ft - ant_ft:.3f}\")\n",
    "\n",
    "def neighbors(model, word, k=5):\n",
    "    try:\n",
    "        return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "print(\"\\n== Nearest Neighbors ==\")\n",
    "\"\"\"\n",
    "seed_words = [\"yax≈üƒ±\",\"pis\",\"√ßox\",\"bahalƒ±\",\"ucuz\",\"m√ºk…ômm…ôl\",\"d…ôh≈ü…ôt\",\n",
    "              \"<PRICE>\",\"<RATING_POS>\"]\n",
    "\"\"\"\n",
    "for w in seed_words:\n",
    "    print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
    "    print(f\"  FT NN for '{w}':\", neighbors(ft, w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df68cb9",
   "metadata": {},
   "source": [
    "#### Synonym/Antonym Similarity\n",
    "Similarity in word embeddings measures how close 2 vectors are in meaning.  \n",
    "Mathematically, it is computed as cosine similarity\n",
    "\n",
    "$ \\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\\\|a\\\\| \\\\|b\\\\|} $.\n",
    "\n",
    "Range from +1 to -1:\n",
    "- +1 means very similar.\n",
    "- 0 means unrelated.\n",
    "- -1 means opposite directions.\n",
    "\n",
    "**Separation** measures how well the model distinguishes between similar and opposite words.\n",
    "$ \\text{Separation} = \\text{mean(similarity of synonyms)} - \\text{mean(similarity of antonyms)} $.\n",
    "\n",
    "If separation is large, we observe that model clearly understand that synonyms are similar than antonyms.  \n",
    "However if it is small model is not good at distinguishing them.\n",
    "\n",
    "When we look at the output, **FastText** is better for both synonym and anthonym similarity. Separation result is small for both models-meaning they do not strongly separate meanings. \n",
    "\n",
    "**NOTE:** Limited corpus size or insufficient domain balance can cause bad results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4dad5",
   "metadata": {},
   "source": [
    "#### Nearest-neighbor quality.\n",
    "The **Nearest-Neighbor (NN)** metric means the words closest to a given word's vector. It is calculated by `Cosine Similarity`.\n",
    "\n",
    "When we look at the output, `FastText` have better results compared to `Word2Vec`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
